{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESeHDr12ucuC"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "from database import engine\n",
        "from models import Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "ZCpmyyWHu4Sa",
        "outputId": "5e03252c-4cd6-4172-8339-167617742d05"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'source\\dataset_instagram-post-scraper_2024-02-08_14-54-37-568.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "Au491ubKu_ip",
        "outputId": "a0841414-5328-4b14-b8e6-d2102208fb5b"
      },
      "outputs": [],
      "source": [
        "# Combine all captions into one large text string\n",
        "text_data = ' '.join(data['caption'].dropna().astype(str))\n",
        "\n",
        "# Convert text to lowercase\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# Remove punctuation and special characters\n",
        "text_data = text_data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
        "\n",
        "# Add common stop words to the default list, if necessary\n",
        "custom_stopwords = set([\n",
        "    'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por',\n",
        "    'mais', 'as', 'dos', 'como', 'mas', 'foi', 'ao', 'ele', 'das', 'tem', 'à', 'seu', 'sua', 'ou', 'ser', 'quando',\n",
        "    'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'era', 'depois',\n",
        "    'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'estão', 'você', 'tinha', 'foram',\n",
        "    'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'têm', 'numa', 'pelos', 'elas', 'há', 'seja', 'qual', 'será',\n",
        "    'nós', 'tenho', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'fosse', 'dele', 'tu', 'te', 'vocês', 'vos',\n",
        "    'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas',\n",
        "    'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos',\n",
        "    'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos',\n",
        "    'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem',\n",
        "    'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam',\n",
        "    'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos',\n",
        "    'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi',\n",
        "    'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos',\n",
        "    'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha',\n",
        "    'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham',\n",
        "    'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria',\n",
        "    'teríamos', 'teriam','é', 'todo', 'dia', 'hoje', 'sobre', 'deu', 'sempre', 'sobre', 'toda', 'todos', 'dia', 'dias','tá', \n",
        "    'todas', 'hoje', 'agora', 'aqui', 'vamos', 'vai', 'tudo', 'vamo', 'vem', 'aí', 'além', 'alem', 'link', 'bio', 'pra', 'junto',\n",
        "      'pode', 'fazer', 'outra', 'ainda', 'assim', 'nesse', 'onde', 'precisa', 'ontem', 'muita', 'cada', 'dessa', 'tbt', 'quer',\n",
        "      'outro', 'nessa', 'vez', 'desse', 'pois', 'desde'\n",
        "])\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud = WordCloud(stopwords=custom_stopwords, background_color='white', width=800, height=400).generate(text_data)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Do not display the axis\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrGRKkqUGR6M",
        "outputId": "14efbccd-6909-4269-8d66-5498a8b28849"
      },
      "outputs": [],
      "source": [
        "# Function to clean and tokenize text\n",
        "def clean_and_tokenize(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize by splitting the text\n",
        "    words = text.split()\n",
        "    # Remove stopwords and lowercase the tokens\n",
        "    return [word.lower() for word in words if word.lower() not in stopwords]\n",
        "\n",
        "# Assuming 'custom_stopwords' is a list of stopwords you want to exclude\n",
        "stopwords = set(custom_stopwords)\n",
        "\n",
        "# Concatenate all captions into a single string\n",
        "all_captions = ' '.join(data['caption'].dropna().astype(str))\n",
        "\n",
        "# Clean and tokenize the concatenated captions\n",
        "tokens = clean_and_tokenize(all_captions)\n",
        "\n",
        "# Count the occurrences of each word\n",
        "word_counts = Counter(tokens)\n",
        "\n",
        "# Get the most common 20 words and their counts\n",
        "top_words = word_counts.most_common(20)\n",
        "\n",
        "# Print the top 0 words and their counts\n",
        "print(\"Top 20 words in the entire database:\")\n",
        "for word, count in top_words:\n",
        "    print(f\"{word}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Led3dUR4FIoF",
        "outputId": "ddd41610-ddd4-4f30-b7b0-055b62edcecf"
      },
      "outputs": [],
      "source": [
        "# Group the dataset by 'ownerUsername'\n",
        "grouped_data = data.groupby('ownerUsername')\n",
        "\n",
        "# Loop through each group\n",
        "for username, group in grouped_data:\n",
        "    # Get the captions for the current username, dropping any missing values and converting to string\n",
        "    captions = group['caption'].dropna().astype(str)\n",
        "\n",
        "    # Combine all captions into a single string, removing punctuation and converting to lowercase\n",
        "    text_data = ' '.join(captions).lower()\n",
        "    text_data = text_data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
        "\n",
        "     # Check if text_data is empty after preprocessing\n",
        "    if text_data.strip():  # This checks if text_data is not just whitespace\n",
        "        wordcloud = WordCloud(stopwords=custom_stopwords, background_color='white', width=800, height=400).generate(text_data) # Generate the word cloud for the current username\n",
        "        plt.figure(figsize=(10, 5)) # Initialize a figure for the current word cloud\n",
        "        # Plot the word cloud\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Word Cloud for {username}')\n",
        "        plt.axis('off')\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No words to plot for {username}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IxxUjwnFwyj",
        "outputId": "4d0fc87d-1528-4661-d6d5-b6eb5ae655a2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import string\n",
        "import pandas as pd \n",
        "\n",
        "# Supondo que 'custom_stopwords' seja uma lista de palavras a serem excluídas\n",
        "stopwords = set(custom_stopwords)\n",
        "\n",
        "# Função para limpar e tokenizar o texto\n",
        "def clean_and_tokenize(text):\n",
        "    # Remover pontuação\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenizar dividindo o texto\n",
        "    words = text.split()\n",
        "    # Remover stopwords e converter os tokens para minúsculo\n",
        "    return [word.lower() for word in words if word.lower() not in stopwords]\n",
        "\n",
        "# Lista para armazenar os resultados\n",
        "results = []\n",
        "\n",
        "# Loop através de cada grupo\n",
        "for username, group in grouped_data:\n",
        "    # Inicializar um objeto Counter para contar as ocorrências das palavras\n",
        "    word_counts = Counter()\n",
        "\n",
        "    # Loop através de cada legenda no grupo\n",
        "    for caption in group['caption'].dropna().astype(str):\n",
        "        # Limpar e tokenizar a legenda, depois atualizar as contagens das palavras\n",
        "        word_counts.update(clean_and_tokenize(caption))\n",
        "\n",
        "    # Obter as 10 palavras mais comuns e suas contagens\n",
        "    top_words = word_counts.most_common(10)\n",
        "\n",
        "    # Adicionar os resultados para o usuário atual na lista de resultados\n",
        "    for word, count in top_words:\n",
        "        results.append({'Usuário': username, 'Palavra': word, 'Contagem': count})\n",
        "\n",
        "# Criar um DataFrame do pandas com os resultados\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Salvar o DataFrame em um arquivo Excel\n",
        "df_results.to_excel('out/resultados_palavras.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Start building the HTML content\n",
        "html_content = \"<html><head><title>Word Clouds</title></head><body>\"\n",
        "\n",
        "for username, group in grouped_data:\n",
        "    captions = group['caption'].dropna().astype(str)\n",
        "    text_data = ' '.join(captions).lower()\n",
        "    text_data = text_data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
        "\n",
        "    if text_data.strip():  # Check if text_data is not empty\n",
        "        wordcloud = WordCloud(stopwords=custom_stopwords, background_color='white', width=800, height=400).generate(text_data)\n",
        "        \n",
        "        # Save the plot to a BytesIO buffer\n",
        "        buffer = BytesIO()\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Word Cloud for {username}')\n",
        "        plt.axis('off')\n",
        "        plt.savefig(buffer, format='png')\n",
        "        plt.close()  # Close the plot to free up memory\n",
        "\n",
        "        # Encode the image as base64\n",
        "        buffer.seek(0)\n",
        "        img_str = base64.b64encode(buffer.read()).decode()\n",
        "\n",
        "        # Add the image to the HTML, encoded as base64\n",
        "        html_content += f'<h1>Word Cloud for {username}</h1>'\n",
        "        html_content += f'<img src=\"data:image/png;base64,{img_str}\"/><br/>'\n",
        "\n",
        "    else:\n",
        "        print(f\"No words to plot for {username}.\")\n",
        "\n",
        "# Finish the HTML file\n",
        "html_content += \"</body></html>\"\n",
        "\n",
        "# Write the HTML content to a file\n",
        "with open('out/word_clouds.html', 'w') as f:\n",
        "    f.write(html_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import io\n",
        "\n",
        "# Create a new Word document\n",
        "doc = Document()\n",
        "\n",
        "for username, group in grouped_data:\n",
        "    captions = group['caption'].dropna().astype(str)\n",
        "    text_data = ' '.join(captions).lower()\n",
        "    text_data = text_data.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
        "\n",
        "    if text_data.strip():  # Check if text_data is not empty\n",
        "        wordcloud = WordCloud(stopwords=custom_stopwords, background_color='white', width=800, height=400).generate(text_data)\n",
        "        \n",
        "        # Save the plot to a bytes object to avoid writing to disk\n",
        "        buf = io.BytesIO()\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Word Cloud for {username}')\n",
        "        plt.axis('off')\n",
        "        plt.savefig(buf, format='png')\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "        # Seek to the start of the BytesIO buffer\n",
        "        buf.seek(0)\n",
        "\n",
        "        # Add a heading with the username\n",
        "        doc.add_heading(f'Word Cloud for {username}', level=1)\n",
        "\n",
        "        # Add the image to the Word document\n",
        "        doc.add_picture(buf, width=Inches(6))\n",
        "\n",
        "        # Add a page break after each word cloud\n",
        "        doc.add_page_break()\n",
        "\n",
        "        # Clear the buffer for the next image\n",
        "        buf.truncate(0)\n",
        "        buf.seek(0)\n",
        "    else:\n",
        "        print(f\"No words to plot for {username}.\")\n",
        "\n",
        "# Save the document\n",
        "doc.save('out/word_clouds.docx')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
